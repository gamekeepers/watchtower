{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c2e7eb-55ce-480f-8f5d-5a9a9551a344",
   "metadata": {},
   "source": [
    "### Document parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cea8387e-789e-4cbf-9c6d-52b0373c0015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in /home/ankush/workplace/tryout_repos/chatbot-rag-app/.venv/lib/python3.11/site-packages (0.13.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: grobid in /home/ankush/workplace/tryout_repos/chatbot-rag-app/.venv/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: bs4<0.0.2,>=0.0.1 in /home/ankush/workplace/tryout_repos/chatbot-rag-app/.venv/lib/python3.11/site-packages (from grobid) (0.0.1)\n",
      "Requirement already satisfied: httpx<0.24.0,>=0.23.0 in /home/ankush/workplace/tryout_repos/chatbot-rag-app/.venv/lib/python3.11/site-packages (from grobid) (0.23.3)\n",
      "Requirement already satisfied: lxml<5.0.0,>=4.9.1 in /home/ankush/workplace/tryout_repos/chatbot-rag-app/.venv/lib/python3.11/site-packages (from grobid) (4.9.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.7.8 in /home/ankush/workplace/tryout_repos/chatbot-rag-app/.venv/lib/python3.11/site-packages (from grobid) (3.9.15)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ankush/workplace/tryout_repos/chatbot-rag-app/.venv/lib/python3.11/site-packages (from bs4<0.0.2,>=0.0.1->grobid) (4.12.3)\n",
      "Requirement already satisfied: certifi in /home/ankush/workplace/tryout_repos/chatbot-rag-app/.venv/lib/python3.11/site-packages (from httpx<0.24.0,>=0.23.0->grobid) (2023.7.22)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /home/ankush/workplace/tryout_repos/chatbot-rag-app/.venv/lib/python3.11/site-packages (from httpx<0.24.0,>=0.23.0->grobid) (0.16.3)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /home/ankush/workplace/tryout_repos/chatbot-rag-app/.venv/lib/python3.11/site-packages (from httpx<0.24.0,>=0.23.0->grobid) (1.5.0)\n",
      "Requirement already satisfied: sniffio in /home/ankush/workplace/tryout_repos/chatbot-rag-app/.venv/lib/python3.11/site-packages (from httpx<0.24.0,>=0.23.0->grobid) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ankush/workplace/tryout_repos/chatbot-rag-app/.venv/lib/python3.11/site-packages (from httpcore<0.17.0,>=0.15.0->httpx<0.24.0,>=0.23.0->grobid) (0.14.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/ankush/workplace/tryout_repos/chatbot-rag-app/.venv/lib/python3.11/site-packages (from httpcore<0.17.0,>=0.15.0->httpx<0.24.0,>=0.23.0->grobid) (3.7.1)\n",
      "Requirement already satisfied: idna in /home/ankush/workplace/tryout_repos/chatbot-rag-app/.venv/lib/python3.11/site-packages (from rfc3986[idna2008]<2,>=1.3->httpx<0.24.0,>=0.23.0->grobid) (3.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ankush/workplace/tryout_repos/chatbot-rag-app/.venv/lib/python3.11/site-packages (from beautifulsoup4->bs4<0.0.2,>=0.0.1->grobid) (2.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "! pip install xmltodict\n",
    "! pip install grobid grobid[json]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "559a3a48-9f4b-48fe-a9c1-62d8aa12f842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ankush/workplace/tryout_repos/chatbot-rag-app/test_notebooks'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "269da4bb471e4415",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parse_pdf_by_grobid is 1.7213678359985352\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "import json\n",
    "from grobid.tei import Parser\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "CURRENT_DIR = os.path.abspath(\"\")\n",
    "\n",
    "\n",
    "def add_folder_to_sys_path(folder_path):\n",
    "    abs_path = f\"{CURRENT_DIR}/{folder_path}\"\n",
    "    if abs_path not in sys.path:\n",
    "        sys.path.append(abs_path)\n",
    "\n",
    "\n",
    "# add api folder to sys path\n",
    "add_folder_to_sys_path(\"../api\")\n",
    "\n",
    "from vector_util import compute_vector, extract_and_map_sparse_vector, tokenizer\n",
    "\n",
    "PAPER_PATH = \"/home/ankush/workplace/papers/KnowlegeGraphs/textrank_emnlp04.pdf\"\n",
    "\n",
    "\n",
    "# timer decorator\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        import time\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"Time taken by {func.__name__} is {end - start}\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@timer\n",
    "def parse_pdf_by_grobid(pdf_path=\"\"):\n",
    "    # request grobid endpoint\n",
    "    try:\n",
    "        GROBID_URL = 'http://localhost:8080'\n",
    "        url = f\"{GROBID_URL}/api/processFulltextDocument\"\n",
    "        xml_content = requests.post(url, files={'input': open(pdf_path, 'rb')}, data={})\n",
    "        parser = Parser(xml_content.text)\n",
    "        article = parser.parse()\n",
    "        return json.loads(article.to_json())  # raises Runtim\n",
    "    except Exception as err:\n",
    "        print(traceback.format_exc())\n",
    "    # return json_content\n",
    "\n",
    "\n",
    "article_content = parse_pdf_by_grobid(PAPER_PATH)\n",
    "# print(article_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a640427d-dd13-45c3-abe6-1bc5e2efcbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Graph-based ranking algorithms like Kleinberg's HITS algorithm (Kleinberg, 1999) or Google's PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web. Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages. In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.\",\n",
       " 'refs': [{'start': 63, 'end': 80, 'marker': 'bibr', 'target': '#b6'},\n",
       "  {'start': 102, 'end': 123, 'marker': 'bibr', 'target': '#b0'}]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# article_content = json.loads(article_content) \n",
    "article_content[\"sections\"][0].keys()\n",
    "article_content[\"sections\"][0][\"title\"]\n",
    "article_content[\"sections\"][0][\"paragraphs\"][0]\n",
    "# article_content[\"sections\"][0][\"paragraphs\"][0].keys()\n",
    "# article_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc629108-14d3-46a3-96a9-9ebad5e6442d",
   "metadata": {},
   "source": [
    "## index paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3e06e33-96db-4cb8-ae27-a1c30a2a980d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Qdrant client setup\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client import models\n",
    "import uuid\n",
    "\n",
    "# Define collection name\n",
    "COLLECTION_NAME = \"literature_collection\"\n",
    "\n",
    "# # Insert sparse vector into Qdrant collection\n",
    "# point_id = 1  # Assign a unique ID for the point\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\")\n",
    "client.recreate_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config={},\n",
    "    sparse_vectors_config={\n",
    "        \"text\": models.SparseVectorParams(\n",
    "            index=models.SparseIndexParams(\n",
    "                on_disk=False,\n",
    "            )\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b27e609-be51-4fb7-996f-3dc8bc32b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "\n",
    "# title = article_content[\"title\"]\n",
    "for section_id, section in enumerate(article_content[\"sections\"]):\n",
    "    section_title = section[\"title\"]\n",
    "    for para_id, para in enumerate(section[\"paragraphs\"]):\n",
    "        vec, tokens = compute_vector(para[\"text\"])\n",
    "        indices = vec.nonzero().numpy().flatten()\n",
    "        values = vec.detach().numpy()[indices]\n",
    "        payload = {\n",
    "            # \"paper_title\": title,\n",
    "            \"section_title\": section_title,\n",
    "            \"page_content\": para[\"text\"]\n",
    "        }\n",
    "        point_id = str(uuid.uuid4())\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[\n",
    "                models.PointStruct(\n",
    "                    id=point_id,\n",
    "                    payload=payload,\n",
    "                    vector={\n",
    "                        \"text\": models.SparseVector(\n",
    "                            indices=indices.tolist(), values=values.tolist()\n",
    "                        )\n",
    "                    },\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b9a8ad-84c8-4615-a263-845e91f012cc",
   "metadata": {},
   "source": [
    "## find similar records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "271ebbc1-04a9-48d1-891a-c03f732e86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing a query vector\n",
    "\n",
    "def get_context_docs(query_text):\n",
    "    # query_text = \"What is pagerank?\"\n",
    "    query_vec, query_tokens = compute_vector(query_text)\n",
    "    query_vec.shape\n",
    "    \n",
    "    query_expansion = extract_and_map_sparse_vector(query_vec, tokenizer)\n",
    "    # print(query_expansion)\n",
    "    \n",
    "    query_indices = query_vec.nonzero().numpy().flatten()\n",
    "    query_values = query_vec.detach().numpy()[query_indices]\n",
    "    \n",
    "    # Searching for similar documents\n",
    "    result = client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query_vector=models.NamedSparseVector(\n",
    "            name=\"text\",\n",
    "            vector=models.SparseVector(\n",
    "                indices=query_indices,\n",
    "                values=query_values,\n",
    "            ),\n",
    "        ),\n",
    "        with_vectors=True,\n",
    "    )\n",
    "    \n",
    "    return [res.payload for res in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdff1cd3-2ad2-46a2-9c45-945807103a12",
   "metadata": {},
   "source": [
    "## make prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "30de2040-87b9-4c46-973b-bae86778cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rag_prompt(question, docs):\n",
    "    context = \"\"\n",
    "    doc_texts = []\n",
    "    for doc in docs:\n",
    "        section_title= doc[\"section_title\"]\n",
    "        doc_texts.append(f\"NAME: {section_title}\\n{doc['page_content']}\")\n",
    "    context = \"\\n---\\n\".join(doc_texts)\n",
    "    prompt = f\"\"\"\n",
    "        Use the following passages and chat history to answer the user's question. \n",
    "        Each passage has a NAME which is the title of the document. After your answer, leave a blank line and then give the source name of the passages you answered from. Put them in a comma separated list, prefixed with SOURCES:.\n",
    "        \n",
    "        Example:\n",
    "        \n",
    "        Question: What is the meaning of life?\n",
    "        Response:\n",
    "        The meaning of life is 42.\n",
    "        \n",
    "        SOURCES: Hitchhiker's Guide to the Galaxy\n",
    "        \n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "        \n",
    "        ----\n",
    "        {context}\n",
    "        ----\n",
    "        \n",
    "        Question: {question}\n",
    "        Response:\n",
    "    \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1fe06-8d56-4dd6-831e-80dd30b371d4",
   "metadata": {},
   "source": [
    "## talk to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5c0440f7-6768-4e29-80c4-9b7daf55ea66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 2.8, '##rank': 2.28, 'algorithm': 1.83, 'work': 1.3, 'algorithms': 1.25, 'calculate': 1.13, 'rank': 1.08, 'method': 0.99, 'works': 0.91, 'index': 0.87, 'step': 0.85, 'function': 0.78, 'ranked': 0.73, 'mechanism': 0.68, 'math': 0.67, 'texts': 0.63, 'java': 0.58, 'fuzzy': 0.55, 'equation': 0.5, ':': 0.38, 'reading': 0.35, 'button': 0.34, 'tracking': 0.33, 'solve': 0.29, 'computer': 0.28, 'generator': 0.27, 'how': 0.26, 'tool': 0.26, 'create': 0.25, 'strategy': 0.24, 'help': 0.22, 'engine': 0.17, 'machine': 0.17, 'search': 0.17, 'error': 0.17, 'accuracy': 0.17, 'word': 0.14, 'graph': 0.14, 'optimization': 0.14, 'editor': 0.13, 'process': 0.11, 'data': 0.09, 'avery': 0.08, 'technique': 0.07, 'connection': 0.04, 'calculated': 0.04, 'useful': 0.03}\n",
      "The TextRank algorithm works by building a graph associated with the text, where each vertex represents a text unit to be ranked. It then scores these text units based on the importance of other text units they are connected to in the graph. The algorithm iteratively ranks the text units based on recommendations made by related text units, with preference given to the recommendations made by the most influential ones. This process approximates the model humans build about a given context in the process of discourse understanding.\n",
      "\n",
      "SOURCES: Why TextRank Works, TextRank for Sentence Extraction, The TextRank Model\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "def get_llm(temperature=0):\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "    return ChatOpenAI(\n",
    "            openai_api_key=OPENAI_API_KEY, streaming=True, temperature=temperature\n",
    "        )\n",
    "\n",
    "query_text = \"How does textrank algorithm work?\"\n",
    "context_docs = get_context_docs(query_text)\n",
    "\n",
    "qa_prompt = make_rag_prompt(query_text, context_docs)\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=qa_prompt\n",
    "    ),\n",
    "]\n",
    "\n",
    "answer = get_llm().invoke(messages)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ca465119-80d3-487e-92b3-926c21024217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'TextRank: Bringing Order into Texts',\n",
       " 'authors': [{'person_name': {'surname': 'Mihalcea', 'first_name': 'Rada'},\n",
       "   'affiliations': [{'department': 'Department of Computer Science',\n",
       "     'institution': 'University of North Texas',\n",
       "     'laboratory': None}],\n",
       "   'email': None},\n",
       "  {'person_name': {'surname': 'Tarau', 'first_name': 'Paul'},\n",
       "   'affiliations': [{'department': 'Department of Computer Science',\n",
       "     'institution': 'University of North Texas',\n",
       "     'laboratory': None}],\n",
       "   'email': 'tarauÂ¡@cs.unt.edu'}],\n",
       " 'date': None,\n",
       " 'ids': None,\n",
       " 'target': None,\n",
       " 'publisher': None,\n",
       " 'journal': None,\n",
       " 'series': None,\n",
       " 'scope': None}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_content.keys()# dict_keys(['bibliography', 'keywords', 'citations', 'sections', 'tables', 'abstract'])\n",
    "article_content[\"bibliography\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
